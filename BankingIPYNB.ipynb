{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install faker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnWAf-dMT8Vi",
        "outputId": "0a7980bd-6f19-4e87-ce82-6eabbf0c7723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading faker-39.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.12/dist-packages (from faker) (2025.3)\n",
            "Downloading faker-39.0.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/2.0 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-39.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "twwDBxtSE5xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oz9YzhgeE5uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qQFlK2ubE5mY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# predictive_banking_project.py\n",
        "# Single-file end-to-end reproducible implementation as requested.\n",
        "\n",
        "# ---------------------------\n",
        "# 0) Imports and global setup\n",
        "# ---------------------------\n",
        "import os                                  # file/directory operations\n",
        "import random                              # random utilities\n",
        "import zipfile                             # for packaging outputs\n",
        "from datetime import datetime, timedelta  # date operations\n",
        "import numpy as np                         # numerical computing\n",
        "import pandas as pd                        # dataframes\n",
        "from faker import Faker                     # synthetic realistic data\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, confusion_matrix\n",
        ")\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from sklearn.impute import SimpleImputer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib                                # save/load models\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")             # keep output tidy\n",
        "\n",
        "RND = 42                                      # reproducible seed\n",
        "random.seed(RND)\n",
        "np.random.seed(RND)\n",
        "fake = Faker()\n",
        "Faker.seed(RND)\n",
        "\n",
        "# Create output directories\n",
        "OUTPUT_DIR = \"project_outputs\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(os.path.join(OUTPUT_DIR, \"figures\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(OUTPUT_DIR, \"models\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(OUTPUT_DIR, \"data\"), exist_ok=True)\n",
        "\n",
        "# ---------------------------\n",
        "# 1) Synthetic datasets\n",
        "# ---------------------------\n",
        "\n",
        "# 1A) Loan dataset generation\n",
        "def generate_loan_dataset(n_customers=10000):\n",
        "    \"\"\"\n",
        "    Generates a synthetic loan dataset with derived features and a probabilistic target.\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    for i in range(n_customers):\n",
        "        customer_id = f\"C{100000 + i}\"\n",
        "        age = int(np.clip(np.random.normal(40, 10), 18, 75))                 # realistic ages\n",
        "        income = int(max(8000, np.random.normal(60000, 30000)))             # annual income\n",
        "        credit_score = int(np.clip(np.random.normal(650, 70), 300, 850))     # credit bureau-like score\n",
        "        loan_amount = int(np.clip(np.random.exponential(15000) + 5000, 1000, income*3))\n",
        "        interest_rate = round(np.clip(np.random.normal(0.12, 0.03), 0.03, 0.25), 4)\n",
        "        loan_term = int(np.random.choice([12, 24, 36, 48, 60]))              # months\n",
        "        # derived features:\n",
        "        income_to_loan = income / (loan_amount + 1)\n",
        "        # monthly EMI using amortization formula\n",
        "        monthly_rate = interest_rate / 12\n",
        "        emi = (loan_amount * monthly_rate) / (1 - (1 + monthly_rate) ** (-loan_term))\n",
        "        tenure_years = int(np.clip(np.random.exponential(3), 0, 40))\n",
        "        # repayment_status probabilistic: high risk if low credit score, low income_to_loan, high emi relative to income\n",
        "        risk_score = (700 - credit_score) * 0.5 + (1 / (income_to_loan + 1)) * 50 + (emi / (income / 12)) * 100\n",
        "        prob_default = 1 / (1 + np.exp(-0.02 * (risk_score - 20)))  # logistic transform => [0,1]\n",
        "        repayment_status = int(np.random.rand() < prob_default)    # 1=default, 0=no default\n",
        "        rows.append({\n",
        "            \"customer_id\": customer_id,\n",
        "            \"age\": age,\n",
        "            \"income\": income,\n",
        "            \"credit_score\": credit_score,\n",
        "            \"loan_amount\": loan_amount,\n",
        "            \"interest_rate\": interest_rate,\n",
        "            \"loan_term\": loan_term,\n",
        "            \"emi\": round(emi, 2),\n",
        "            \"income_to_loan\": round(income_to_loan, 4),\n",
        "            \"tenure_years\": tenure_years,\n",
        "            \"repayment_status\": repayment_status\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "loan_df = generate_loan_dataset(n_customers=8000)\n",
        "loan_df.to_csv(os.path.join(OUTPUT_DIR, \"data\", \"loan_data.csv\"), index=False)\n",
        "\n",
        "# 1B) Transactions dataset generation (for segmentation)\n",
        "def generate_transactions(customers, avg_txn_per_customer=80, months=24):\n",
        "    \"\"\"\n",
        "    Generates a transactional history for each customer over the past `months`.\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    start_date = datetime.now() - timedelta(days=30*months)\n",
        "    txn_types = [\"deposit\", \"withdrawal\", \"payment\", \"transfer\", \"purchase\"]\n",
        "    for cust in customers:\n",
        "        n_txn = max(5, int(np.random.poisson(avg_txn_per_customer)))\n",
        "        for t in range(n_txn):\n",
        "            txn_id = f\"T{random.randint(10000000,99999999)}\"\n",
        "            txn_type = random.choices(txn_types, weights=[0.2,0.2,0.25,0.15,0.2])[0]\n",
        "            # amount depends on type and customer income distribution (fetch income)\n",
        "            income = int(loan_df.loc[loan_df['customer_id'] == cust, 'income'].values[0])\n",
        "            if txn_type == \"deposit\":\n",
        "                amount = max(50, np.random.normal(income/12, income/24))\n",
        "            elif txn_type == \"withdrawal\":\n",
        "                amount = max(10, np.random.exponential(income/120))\n",
        "            elif txn_type == \"payment\":\n",
        "                amount = max(10, np.random.exponential(200))\n",
        "            elif txn_type == \"transfer\":\n",
        "                amount = max(5, np.random.exponential(income/300))\n",
        "            else:  # purchase\n",
        "                amount = max(5, np.random.exponential(80))\n",
        "            txn_date = start_date + timedelta(days=random.randint(0, months*30))\n",
        "            rows.append({\n",
        "                \"customer_id\": cust,\n",
        "                \"transaction_id\": txn_id,\n",
        "                \"transaction_amount\": round(float(amount), 2),\n",
        "                \"transaction_type\": txn_type,\n",
        "                \"transaction_date\": txn_date.strftime(\"%Y-%m-%d\")\n",
        "            })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "txn_df = generate_transactions(loan_df['customer_id'].tolist(), avg_txn_per_customer=60, months=24)\n",
        "txn_df.to_csv(os.path.join(OUTPUT_DIR, \"data\", \"transactions.csv\"), index=False)\n",
        "\n",
        "# 1C) Interaction dataset for recommendation engine\n",
        "def generate_interactions(customers, n_products=200, avg_interactions_per_customer=40):\n",
        "    \"\"\"\n",
        "    Generates customer-product interactions with types (viewed, clicked, purchased).\n",
        "    \"\"\"\n",
        "    products = [f\"P{1000 + i}\" for i in range(n_products)]\n",
        "    rows = []\n",
        "    for cust in customers:\n",
        "        n_inter = max(5, int(np.random.poisson(avg_interactions_per_customer)))\n",
        "        for _ in range(n_inter):\n",
        "            product_id = random.choice(products)\n",
        "            interaction_type = random.choices([\"viewed\", \"clicked\", \"purchased\"], weights=[0.7, 0.2, 0.1])[0]\n",
        "            # weight for collaborative filtering (purchased=3, clicked=1.5, viewed=1)\n",
        "            weight = {\"viewed\": 1.0, \"clicked\": 1.5, \"purchased\": 3.0}[interaction_type]\n",
        "            # pick interaction date in last 2 years\n",
        "            interaction_date = datetime.now() - timedelta(days=random.randint(0, 730))\n",
        "            rows.append({\n",
        "                \"customer_id\": cust,\n",
        "                \"product_id\": product_id,\n",
        "                \"interaction_type\": interaction_type,\n",
        "                \"weight\": weight,\n",
        "                \"interaction_date\": interaction_date.strftime(\"%Y-%m-%d\")\n",
        "            })\n",
        "    return pd.DataFrame(rows), products\n",
        "\n",
        "interactions_df, product_list = generate_interactions(loan_df['customer_id'].tolist(), n_products=300, avg_interactions_per_customer=30)\n",
        "interactions_df.to_csv(os.path.join(OUTPUT_DIR, \"data\", \"interactions.csv\"), index=False)\n",
        "\n",
        "# ---------------------------\n",
        "# 2) Feature engineering & EDA\n",
        "# ---------------------------\n",
        "\n",
        "# 2A) Loan EDA & derived augmentations (already has some derived fields)\n",
        "loan_df.describe(include='all').to_csv(os.path.join(OUTPUT_DIR, \"figures\", \"loan_summary_stats.csv\"))\n",
        "# plot distributions: credit_score, income, loan_amount\n",
        "plt.figure(figsize=(10,4))\n",
        "sns.histplot(loan_df['credit_score'], bins=30, kde=True)\n",
        "plt.title(\"Credit Score Distribution\")\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"figures\", \"credit_score_dist.png\"))\n",
        "plt.close()\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "sns.histplot(loan_df['income'], bins=40, kde=True)\n",
        "plt.title(\"Income Distribution\")\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"figures\", \"income_dist.png\"))\n",
        "plt.close()\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "sns.histplot(loan_df['loan_amount'], bins=40, kde=True)\n",
        "plt.title(\"Loan Amount Distribution\")\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"figures\", \"loan_amount_dist.png\"))\n",
        "plt.close()\n",
        "\n",
        "# correlation matrix for loan dataset\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(loan_df[['age','income','credit_score','loan_amount','interest_rate','loan_term','emi','income_to_loan']].corr(), annot=True, fmt=\".2f\")\n",
        "plt.title(\"Loan Features Correlation\")\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"figures\", \"loan_corr.png\"))\n",
        "plt.close()\n",
        "\n",
        "# 2B) Transaction-level feature engineering (RFM-like)\n",
        "txn_df['transaction_date'] = pd.to_datetime(txn_df['transaction_date'])\n",
        "# calculate recency (days since last txn), frequency, monetary (sum) per customer\n",
        "snapshot_date = datetime.now()\n",
        "agg_txn = txn_df.groupby('customer_id').agg(\n",
        "    recency_days = ('transaction_date', lambda x: (snapshot_date - x.max()).days),\n",
        "    frequency = ('transaction_id', 'count'),\n",
        "    monetary = ('transaction_amount', 'sum'),\n",
        "    avg_txn_amount = ('transaction_amount', 'mean')\n",
        ").reset_index()\n",
        "agg_txn.to_csv(os.path.join(OUTPUT_DIR, \"data\", \"customer_txn_agg.csv\"), index=False)\n",
        "\n",
        "# Merge loan_df with txn aggregates for downstream tasks\n",
        "customer_profile = loan_df.merge(agg_txn, on='customer_id', how='left').fillna({\n",
        "    'recency_days': 999, 'frequency':0, 'monetary':0, 'avg_txn_amount':0\n",
        "})\n",
        "\n",
        "# Save a profile snapshot\n",
        "customer_profile.to_csv(os.path.join(OUTPUT_DIR, \"data\", \"customer_profile.csv\"), index=False)\n",
        "\n",
        "# Quick EDA plots for segmentation variables\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.histplot(customer_profile['frequency'], bins=40, kde=False)\n",
        "plt.title(\"Transaction Frequency per Customer\")\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"figures\", \"frequency_dist.png\"))\n",
        "plt.close()\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.scatterplot(data=customer_profile, x='frequency', y='monetary')\n",
        "plt.title(\"Frequency vs Monetary\")\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"figures\", \"freq_vs_monetary.png\"))\n",
        "plt.close()\n",
        "\n",
        "# 2C) Interaction-level summary for recommender\n",
        "# aggregate weight per (customer,product)\n",
        "inter_agg = interactions_df.groupby(['customer_id','product_id'])['weight'].sum().reset_index()\n",
        "inter_agg.to_csv(os.path.join(OUTPUT_DIR, \"data\", \"interactions_agg.csv\"), index=False)\n",
        "\n",
        "# ---------------------------\n",
        "# 3) Supervised Learning: Loan Default Prediction\n",
        "# ---------------------------\n",
        "\n",
        "# 3A) Prepare features and target\n",
        "loan_features = ['age','income','credit_score','loan_amount','interest_rate','loan_term','emi','income_to_loan','tenure_years','frequency','monetary','avg_txn_amount']\n",
        "# ensure merged columns exist in loan_df by merging customer_profile info\n",
        "loan_model_df = loan_df.merge(agg_txn, on='customer_id', how='left').fillna({'recency_days':999,'frequency':0,'monetary':0,'avg_txn_amount':0})\n",
        "X = loan_model_df[loan_features]\n",
        "y = loan_model_df['repayment_status']\n",
        "\n",
        "# 3B) Preprocessing + Model pipeline\n",
        "numeric_features = loan_features\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', numeric_transformer, numeric_features)\n",
        "])\n",
        "\n",
        "rf_pipe = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('clf', RandomForestClassifier(random_state=RND, n_jobs=-1))\n",
        "])\n",
        "\n",
        "# 3C) Train-test split and hyperparameter tuning\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RND)\n",
        "\n",
        "param_grid = {\n",
        "    'clf__n_estimators': [100, 200],\n",
        "    'clf__max_depth': [6, 12, None],\n",
        "    'clf__min_samples_split': [2, 5]\n",
        "}\n",
        "grid = GridSearchCV(rf_pipe, param_grid, cv=3, scoring='roc_auc', n_jobs=-1, verbose=0)\n",
        "grid.fit(X_train, y_train)\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "# Save best model\n",
        "joblib.dump(best_model, os.path.join(OUTPUT_DIR, \"models\", \"best_loan_model.pkl\"))\n",
        "\n",
        "# 3D) Evaluation\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_proba = best_model.predict_proba(X_test)[:,1]\n",
        "metrics = {\n",
        "    \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "    \"precision\": precision_score(y_test, y_pred, zero_division=0),\n",
        "    \"recall\": recall_score(y_test, y_pred, zero_division=0),\n",
        "    \"f1\": f1_score(y_test, y_pred, zero_division=0),\n",
        "    \"roc_auc\": roc_auc_score(y_test, y_proba)\n",
        "}\n",
        "pd.Series(metrics).to_csv(os.path.join(OUTPUT_DIR, \"figures\", \"loan_model_metrics.csv\"))\n",
        "\n",
        "# Confusion matrix plot\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Loan Default Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"figures\", \"loan_confusion_matrix.png\"))\n",
        "plt.close()\n",
        "\n",
        "# Feature importances\n",
        "# Extract numeric pipeline and get importances aligned with numeric features\n",
        "rf = best_model.named_steps['clf']\n",
        "importances = rf.feature_importances_\n",
        "fi = pd.Series(importances, index=numeric_features).sort_values(ascending=False)\n",
        "fi.to_csv(os.path.join(OUTPUT_DIR, \"figures\", \"feature_importances.csv\"))\n",
        "plt.figure(figsize=(8,4))\n",
        "fi.head(10).plot.bar()\n",
        "plt.title(\"Top 10 Feature Importances\")\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"figures\", \"feature_importance_top10.png\"))\n",
        "plt.close()\n",
        "\n",
        "# ---------------------------\n",
        "# 4) Unsupervised Learning: Customer Segmentation\n",
        "# ---------------------------\n",
        "\n",
        "# 4A) Prepare features for clustering (RFM + credit score + income)\n",
        "cluster_features = ['recency_days','frequency','monetary','avg_txn_amount','credit_score','income']\n",
        "cluster_df = customer_profile[cluster_features].copy()\n",
        "# handle missing\n",
        "cluster_df['monetary'] = cluster_df['monetary'].fillna(0)\n",
        "# scale\n",
        "scaler = StandardScaler()\n",
        "cluster_scaled = scaler.fit_transform(cluster_df)\n",
        "\n",
        "# 4B) Choose K via elbow and silhouette\n",
        "sse = []\n",
        "sil_scores = []\n",
        "K_range = range(2,9)\n",
        "for k in K_range:\n",
        "    km = KMeans(n_clusters=k, random_state=RND, n_init=10)\n",
        "    labels = km.fit_predict(cluster_scaled)\n",
        "    sse.append(km.inertia_)\n",
        "    sil_scores.append(silhouette_score(cluster_scaled, labels))\n",
        "\n",
        "# save elbow & silhouette plot\n",
        "plt.figure(figsize=(8,3))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(K_range, sse, marker='o')\n",
        "plt.title('Elbow (SSE)')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('SSE')\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(K_range, sil_scores, marker='o')\n",
        "plt.title('Silhouette Score by k')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Silhouette')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"figures\", \"k_selection.png\"))\n",
        "plt.close()\n",
        "\n",
        "# choose k with highest silhouette\n",
        "best_k = K_range[int(np.argmax(sil_scores))]\n",
        "km_final = KMeans(n_clusters=best_k, random_state=RND, n_init=20)\n",
        "cluster_labels = km_final.fit_predict(cluster_scaled)\n",
        "customer_profile['cluster'] = cluster_labels\n",
        "\n",
        "# cluster profiling\n",
        "cluster_summary = customer_profile.groupby('cluster').agg({\n",
        "    'customer_id':'count',\n",
        "    'recency_days':'mean',\n",
        "    'frequency':'mean',\n",
        "    'monetary':'mean',\n",
        "    'credit_score':'mean',\n",
        "    'income':'mean'\n",
        "}).rename(columns={'customer_id':'size'}).reset_index()\n",
        "cluster_summary.to_csv(os.path.join(OUTPUT_DIR, \"figures\", \"cluster_summary.csv\"), index=False)\n",
        "\n",
        "# PCA plot for visualization\n",
        "pca = PCA(n_components=2, random_state=RND)\n",
        "proj = pca.fit_transform(cluster_scaled)\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(proj[:,0], proj[:,1], c=cluster_labels, cmap='tab10', s=10)\n",
        "plt.title(\"Customer Segments (PCA 2D)\")\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"figures\", \"clusters_pca.png\"))\n",
        "plt.close()\n",
        "\n",
        "joblib.dump(km_final, os.path.join(OUTPUT_DIR, \"models\", \"kmeans_customers.pkl\"))\n",
        "\n",
        "# ---------------------------\n",
        "# 5) Recommendation Engine\n",
        "# ---------------------------\n",
        "\n",
        "# 5A) Build user-item matrix\n",
        "# Use inter_agg dataframe with weight sums\n",
        "user_item = inter_agg.pivot(index='customer_id', columns='product_id', values='weight').fillna(0)\n",
        "\n",
        "# 5B) Item-based collaborative filtering using cosine similarity (simple)\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "item_matrix = user_item.T  # items x users\n",
        "# to reduce dimensionality / noise, use TruncatedSVD on item-user matrix\n",
        "svd_k = 50\n",
        "svd = TruncatedSVD(n_components=min(svd_k, min(item_matrix.shape)-1), random_state=RND)\n",
        "item_factors = svd.fit_transform(item_matrix)  # item latent vectors\n",
        "# compute item-item similarity in latent space\n",
        "item_similarity = cosine_similarity(item_factors)\n",
        "\n",
        "# map product index to similarity matrix\n",
        "product_index = list(item_matrix.index)\n",
        "item_sim_df = pd.DataFrame(item_similarity, index=product_index, columns=product_index)\n",
        "\n",
        "# 5C) Recommendation function: given customer, score items by similarity to items they've interacted with\n",
        "def recommend_items_for_customer(customer_id, top_k=10, user_item_matrix=user_item, sim_df=item_sim_df):\n",
        "    \"\"\"\n",
        "    Returns top_k recommended products for a given customer_id (not including already interacted items).\n",
        "    \"\"\"\n",
        "    if customer_id not in user_item_matrix.index:\n",
        "        return []  # cold start: no interactions\n",
        "    user_vector = user_item_matrix.loc[customer_id]\n",
        "    interacted = user_vector[user_vector > 0].index.tolist()\n",
        "    # score candidate items: sum(similarity * user_interaction_weight) across items user interacted with\n",
        "    scores = {}\n",
        "    for candidate in sim_df.columns:\n",
        "        if candidate in interacted:\n",
        "            continue\n",
        "        # dot product: similarity to each item * user's weight for that item\n",
        "        sim_to_user_items = sim_df.loc[candidate, interacted].values if len(interacted)>0 else np.array([])\n",
        "        user_weights = user_vector[interacted].values if len(interacted)>0 else np.array([])\n",
        "        score = float(np.dot(sim_to_user_items, user_weights)) if len(sim_to_user_items)>0 else 0.0\n",
        "        scores[candidate] = score\n",
        "    # return top_k sorted by score\n",
        "    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
        "    return [p for p, s in ranked]\n",
        "\n",
        "# 5D) Offline evaluation: Precision@K, Recall@K\n",
        "def precision_recall_at_k(test_interactions, train_interactions, user_item_matrix_train, sim_df, k=10):\n",
        "    \"\"\"\n",
        "    test_interactions: DataFrame of (customer_id, product_id) that occurred in test set\n",
        "    train_interactions: DataFrame of (customer_id, product_id) in train set\n",
        "    \"\"\"\n",
        "    # build mapping: for each user, true set of test items and train items\n",
        "    true_map = test_interactions.groupby('customer_id')['product_id'].apply(set).to_dict()\n",
        "    train_map = train_interactions.groupby('customer_id')['product_id'].apply(set).to_dict()\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    users = list(true_map.keys())\n",
        "    for u in users:\n",
        "        recommended = recommend_items_for_customer(u, top_k=k, user_item_matrix=user_item_matrix_train, sim_df=sim_df)\n",
        "        true_items = true_map.get(u, set())\n",
        "        if len(true_items) == 0:\n",
        "            continue\n",
        "        recommended_set = set(recommended)\n",
        "        n_relevant = len(recommended_set & true_items)\n",
        "        precisions.append(n_relevant / k)\n",
        "        recalls.append(n_relevant / len(true_items))\n",
        "    return np.mean(precisions) if precisions else 0.0, np.mean(recalls) if recalls else 0.0\n",
        "\n",
        "# Create a train/test split on interactions at customer level (time-based split would be better; here a pseudo split)\n",
        "# Split interactions_df by date: last 30% of interactions per user as \"test\"\n",
        "interactions_df['interaction_date'] = pd.to_datetime(interactions_df['interaction_date'])\n",
        "interactions_df = interactions_df.sort_values('interaction_date')\n",
        "train_rows = []\n",
        "test_rows = []\n",
        "for cust, grp in interactions_df.groupby('customer_id'):\n",
        "    n = len(grp)\n",
        "    split = max(1, int(0.7 * n))\n",
        "    train_rows.append(grp.iloc[:split])\n",
        "    test_rows.append(grp.iloc[split:])\n",
        "train_df = pd.concat(train_rows)\n",
        "test_df = pd.concat(test_rows) if len(test_rows) else pd.DataFrame(columns=interactions_df.columns)\n",
        "\n",
        "# build train user_item matrix\n",
        "train_agg = train_df.groupby(['customer_id','product_id'])['weight'].sum().reset_index()\n",
        "train_ui = train_agg.pivot(index='customer_id', columns='product_id', values='weight').fillna(0)\n",
        "\n",
        "# rebuild SVD factors on train set items\n",
        "item_matrix_train = train_ui.T\n",
        "svd_train = TruncatedSVD(n_components=min(50, min(item_matrix_train.shape)-1), random_state=RND)\n",
        "item_factors_train = svd_train.fit_transform(item_matrix_train)\n",
        "item_sim_train = cosine_similarity(item_factors_train)\n",
        "product_index_train = list(item_matrix_train.index)\n",
        "sim_df_train = pd.DataFrame(item_sim_train, index=product_index_train, columns=product_index_train)\n",
        "\n",
        "# Evaluate Precision@10 and Recall@10\n",
        "prec10, rec10 = precision_recall_at_k(test_df[['customer_id','product_id']], train_df[['customer_id','product_id']], train_ui, sim_df_train, k=10)\n",
        "pd.Series({\"prec@10\":prec10, \"recall@10\":rec10}).to_csv(os.path.join(OUTPUT_DIR, \"figures\", \"recommender_metrics.csv\"))\n",
        "\n",
        "# Save recommender artifacts\n",
        "joblib.dump(svd_train, os.path.join(OUTPUT_DIR, \"models\", \"recommender_svd.pkl\"))\n",
        "sim_df_train.to_csv(os.path.join(OUTPUT_DIR, \"models\", \"item_similarity.csv\"))\n",
        "\n",
        "# ---------------------------\n",
        "# 6) Packaging & brief report generation\n",
        "# ---------------------------\n",
        "\n",
        "# Save small textual reports\n",
        "with open(os.path.join(OUTPUT_DIR, \"report_summary.txt\"), \"w\") as f:\n",
        "    f.write(\"Project: Predictive Analytics and Recommendation Systems in Banking\\n\")\n",
        "    f.write(\"Contents:\\n\")\n",
        "    f.write(\"- Synthetic loan, transactions, interactions datasets\\n\")\n",
        "    f.write(\"- Loan model: RandomForest (grid-searched). Metrics saved in figures/loan_model_metrics.csv\\n\")\n",
        "    f.write(\"- Clustering: KMeans with K selected by silhouette. Cluster summary in figures/cluster_summary.csv\\n\")\n",
        "    f.write(\"- Recommender: Item-item via SVD + cosine similarity. Metrics in figures/recommender_metrics.csv\\n\")\n",
        "\n",
        "# Create a zip of outputs\n",
        "zipf = os.path.join(OUTPUT_DIR, \"project_bundle.zip\")\n",
        "with zipfile.ZipFile(zipf, 'w', zipfile.ZIP_DEFLATED) as z:\n",
        "    for root, _, files in os.walk(OUTPUT_DIR):\n",
        "        for file in files:\n",
        "            if file.endswith(\".zip\"):  # don't include the zip itself repeatedly\n",
        "                continue\n",
        "            full = os.path.join(root, file)\n",
        "            # store relative path inside zip\n",
        "            arcname = os.path.relpath(full, OUTPUT_DIR)\n",
        "            z.write(full, arcname)\n",
        "\n",
        "print(\"All done. Outputs saved to\", OUTPUT_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fveAdP7Op4IU",
        "outputId": "87e09eec-9639-46fb-fc0d-85f575cd56c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All done. Outputs saved to project_outputs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "model = joblib.load(\"best_loan_model.pkl\")\n",
        "sample = pd.read_csv(\"loan_data.csv\").head(5)\n",
        "\n",
        "sample[\"frequency\"] = 0\n",
        "sample[\"monetary\"] = 0\n",
        "sample[\"avg_txn_amount\"] = 0\n",
        "\n",
        "X_sample = sample.drop(columns=[\"customer_id\",\"repayment_status\"])\n",
        "pred = model.predict(X_sample)\n",
        "proba = model.predict_proba(X_sample)[:,1]\n",
        "\n",
        "print(pred)\n",
        "print(proba)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NphgceivGSA7",
        "outputId": "bf86542e-11c3-40fe-d021-901bba8aca7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 1 1 1]\n",
            "[0.51270286 0.774105   0.89993888 0.85013622 0.536704  ]\n"
          ]
        }
      ]
    }
  ]
}